{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56db649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in c:\\users\\atife\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\atife\\anaconda3\\lib\\site-packages (from folium) (2.25.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\atife\\anaconda3\\lib\\site-packages (from folium) (1.20.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\atife\\anaconda3\\lib\\site-packages (from folium) (2.11.3)\n",
      "Requirement already satisfied: branca>=0.3.0 in c:\\users\\atife\\anaconda3\\lib\\site-packages (from folium) (0.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\atife\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (1.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\atife\\anaconda3\\lib\\site-packages (from requests->folium) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\atife\\anaconda3\\lib\\site-packages (from requests->folium) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\atife\\anaconda3\\lib\\site-packages (from requests->folium) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\atife\\anaconda3\\lib\\site-packages (from requests->folium) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import findspark\n",
    "import pyspark\n",
    "!pip install folium\n",
    "import folium\n",
    "WhoIsWorking=\"Atif\"\n",
    "if(WhoIsWorking==\"Daan\"):\n",
    "    findspark.init(\"c:/Users/Eigenaar/spark-3.1.2-bin-hadoop3.2\")\n",
    "else:\n",
    "    findspark.init(\"C:\\\\spark-3.1.2-bin-hadoop3.2\\\\spark-3.1.2-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3412c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import avg\n",
    "from ipywidgets import widgets\n",
    "from pyspark.sql.functions import col\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import count\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35da9807",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-409cf9706472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e51595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating schema's\n",
    "schema_pubs = StructType([\n",
    "    StructField(\"Food Standard Agency's ID\", StringType(), nullable=True),\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"address\", StringType(), nullable=True),\n",
    "    StructField(\"postcode\", StringType(), nullable=True),\n",
    "    StructField(\"easting\", DoubleType(), nullable=True),\n",
    "    StructField(\"northing\", DoubleType(), nullable=True),\n",
    "    StructField(\"latitude\", DoubleType(), nullable=False),\n",
    "    StructField(\"longitude\", DoubleType(), nullable=False),\n",
    "    StructField(\"local_authority\", StringType(), nullable=True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34aef19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-de164d67baa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpubsData_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'header'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema_pubs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'open_pubs.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0maccidentData_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'header'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../../Desktop/Data mining/Accidents0514.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mpubsData_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'header'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema_pubs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'open_pubs.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcasualtiesData_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'header'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../../Desktop/Data mining/Casualties0514.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# importing data \n",
    "\n",
    "### casualtiesData_df geeft meer info over het slachtoffer\n",
    "if(WhoIsWorking==\"Daan\"):\n",
    "    casualtiesData_df = spark.read.format('csv').option('header', True).load('Kaggle_datasets/Casualties0514.csv')\n",
    "    accidentData_df = spark.read.format('csv').option('header',True).load('Kaggle_datasets/Accidents0514.csv')\n",
    "    pubsData_df = spark.read.format('csv').option('header',False).schema(schema_pubs).load('open_pubs.csv')\n",
    "else:\n",
    "    accidentData_df = spark.read.format('csv').option('header',True).load('../../../Desktop/Data mining/Accidents0514.csv')\n",
    "    pubsData_df = spark.read.format('csv').option('header',False).schema(schema_pubs).load('open_pubs.csv')\n",
    "    casualtiesData_df = spark.read.format('csv').option('header', True).load('../../../Desktop/Data mining/Casualties0514.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter de pub data set\n",
    "counterRowsBeforePubs=pubsData_df.count()\n",
    "pubsData_df=pubsData_df.dropna(subset=[\"latitude\",\"longitude\"])\n",
    "print(counterRowsBeforePubs-pubsData_df.count(),\" rijen zijn verwijderd omdat ze lege waarden hadden.\")\n",
    "### nutteloze kolommen zoals Pedestrian_Crossing,Special_Condition_at_site, Carriageway_Hazards,  want volgens Kaggle hebben bijna alle records dezelfde data dus veel gaat deze kolom ons niet opleveren alleen maar een langere run time. heb ook Location_Northing_OSGR, Location_Easting_OSGR, Local_Authority_(District), Local_Authority_(Highway)1st_Road_Class, 1st_Road_Number, 2nd_Road_Class, 2nd_Road_Number, Pedestrian_Crossing-Human_Control, Pedestrian_Crossing-Physical_Facilities\n",
    "accidentData_df = accidentData_df.drop(\"Pedestrian_Crossing\", \"Special_Condition_at_site\",\"Local_Authority_(Highway)\", \"Carriageway_Hazards\",\"Location_Northing_OSGR\", \"Location_Easting_OSGR\", \"Local_Authority_(District)\", \"Local_Authority_(Highway)1st_Road_Class\", \"1st_Road_Number\", \"2nd_Road_Class\", \"2nd_Road_Number\", \"Pedestrian_Crossing-Human_Control\", \"Pedestrian_Crossing-Physical_Facilities\")\n",
    "### In casualtiesData heb ik ook een paar kolommen verwijderd => Pedestian_Location, Pedestrian_Movement, Bus_or_Coach_, Vehicle_Reference, Casualty_Reference, Age_Band_of_Casualty, Pedestrian_Road_Maintenance_Worker, Casualty_Home_Area_Type\n",
    "casualtiesData_df = casualtiesData_df.drop(\"Pedestian_Location\", \"Pedestrian_Movement\", \"Bus_or_Coach_Passenger\", \"Vehicle_Reference\", \"Casualty_Reference\", \"Age_Band_of_Casualty\", \"Pedestrian_Road_Maintenance_Worker\",\"Casualty_Home_Area_Type\" )\n",
    "print(\"Kolommen Vehicle_Reference en Casualty_Reference zijn zeer nuttige gegevens maar die waarden komen precies niet overeen met die tabel op Kaggle dus heb ze weggelaten. Kan dat ik verkeerd ben maar heb niets gevonden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816441ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter de accident data set\n",
    "counterRowsBeforePubs=accidentData_df.count()\n",
    "accidentData_df=accidentData_df.na.drop()\n",
    "print(counterRowsBeforePubs-accidentData_df.count(),\" rijen zijn verwijderd omdat ze lege waarden hadden.\")\n",
    "\n",
    "### Filter de casaulties data\n",
    "counterRowsBeforeCasaul=casualtiesData_df.count()\n",
    "casualtiesData_df=casualtiesData_df.na.drop()\n",
    "print(counterRowsBeforeCasaul-casualtiesData_df.count(),\" rijen zijn verwijderd omdat ze lege waarden hadden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc29c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidentData_df.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubsData_df.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec46fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table\n",
    "pubsData_df.registerTempTable('pubsTable')\n",
    "accidentData_df.registerTempTable('accidentTable')\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sqlContext = SQLContext(spark)#Running Query\n",
    "#Running Query\n",
    "df1 = sqlContext.sql(\"SELECT COUNT(local_authority) as count,local_authority from pubsTable group by local_authority order by count\")\n",
    "df2 = sqlContext.sql(\"SELECT COUNT(Number_of_Casualties) as count_casulties,SPEED_LIMIT FROM accidentTable group by SPEED_LIMIT order by SPEED_LIMIT\")\n",
    "## type sql dataframe .collect to get list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_casulties_speed=[]\n",
    "list_speed_casulties=[]\n",
    "for x in df2.collect():\n",
    "    list_casulties_speed.append(x[0])\n",
    "    list_speed_casulties.append(int(x[1]))\n",
    "print(list_casulties_speed)\n",
    "plt.bar(list_speed_casulties,list_casulties_speed)\n",
    "plt.title(\"Aantal gevallen per snelheids drempel\")\n",
    "plt.xlabel('Snelheid')\n",
    "plt.ylabel('Aantal Slachtoffers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_authority=[]\n",
    "for row in df1.collect():\n",
    "    local_authority.append(row.local_authority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6aaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_local_authority = widgets.Dropdown(\n",
    "    options=local_authority,\n",
    "    value='Isles of Scilly',\n",
    "    description='Authority:',\n",
    ")\n",
    "\n",
    "def load_map_points(local_authority):\n",
    "    df1 = sqlContext.sql(\"SELECT * from pubsTable where local_authority='\"+local_authority+\"'\")\n",
    "    local_authority_coordinates = (df1.collect()[0].latitude,df1.collect()[0].longitude)\n",
    "    map = folium.Map(location=local_authority_coordinates, zoom_start=11.5)\n",
    "    ## testing \n",
    "    counter=0\n",
    "    for row in df1.collect():\n",
    "        folium.Marker(location = [row.latitude,row.longitude]).add_to(map)\n",
    "    display(map)\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        load_map_points(change['new'])\n",
    "choose_local_authority.observe(on_change)\n",
    "display(display(choose_local_authority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### twee dataframes in één dataframe joinen m.b.v. ID(Accident_Index). Beide dataframes hebben evenveel records.\n",
    "joined = accidentData_df.alias(\"A\").join(casualtiesData_df.alias(\"B\"),col(\"A.Accident_Index\") == col(\"B.Accident_Index\"),\"inner\").select(col(\"A.Accident_Index\"), col(\"A.Longitude\"),col(\"A.Latitude\"), col(\"A.Police_Force\"), col(\"A.Accident_Severity\"), col(\"A.Number_of_Vehicles\"), col(\"A.Number_of_Casualties\"), col(\"A.Date\"), col(\"A.Day_of_Week\"),col(\"A.Time\"), col(\"A.1st_Road_Class\"), col(\"A.Road_Type\"), col(\"A.Speed_limit\"), col(\"A.Junction_Detail\"), col(\"A.Junction_Control\"), col(\"A.Light_Conditions\"), col(\"A.Weather_Conditions\"), col(\"A.Road_Surface_Conditions\"), col(\"A.Special_Conditions_at_Site\"), col(\"A.Urban_or_Rural_Area\"), col(\"A.Did_Police_Officer_Attend_Scene_of_Accident\"), col(\"A.LSOA_of_Accident_Location\"), col(\"B.Casualty_Class\"), col(\"B.Sex_of_Casualty\"), col(\"B.Age_of_Casualty\"), col(\"B.Casualty_Severity\"), col(\"B.Pedestrian_Location\"), col(\"B.Car_Passenger\"), col(\"B.Casualty_Type\")).toDF(\"Accident_Index\", \"Longitude\",\"Latitude\",\"Police_Force\", \"Accident_Severity\",\"Number_of_Vehicles\", \"Number_of_Casualties\", \"Date\", \"Day_of_Week\",\"Time\",\"1st_Road_Class\", \"Road_Type\", \"Speed_limit\",\"Junction_Detail\", \"Junction_Control\",\"Light_Conditions\", \"Weather_Conditions\",\"Road_Surface_Conditions\",\"Special_Conditions_at_Site\",\"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\", \"LSOA_of_Accident_Location\",\"Casualty_Class\",\"Sex_of_Casualty\",\"Age_of_Casualty\",\"Casualty_Severity\",\"Pedestrian_Location\",\"Car_Passenger\",\"Casualty_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Soorten voertuigen\n",
    "\n",
    "joined.groupby(\"Casualty_Type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = joined.where(\"Casualty_Type!=16\")\n",
    "joined = joined.where(\"Casualty_Type!=17\")\n",
    "joined = joined.where(\"Casualty_Type!=18\")\n",
    "joined = joined.where(\"Casualty_Type!=8\")\n",
    "joined = joined.where(\"Casualty_Type!=10\")\n",
    "joined = joined.where(\"Casualty_Type!=11\")\n",
    "joined = joined.where(\"Casualty_Type!=19\")\n",
    "joined = joined.where(\"Casualty_Type!=20\")\n",
    "joined = joined.where(\"Casualty_Type!=21\")\n",
    "joined = joined.where(\"Casualty_Type!=90\")\n",
    "joined = joined.where(\"Casualty_Type!=98\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99250a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Casualty Class\n",
    "joined.groupby(\"Casualty_Class\").count().show()\n",
    "print(\"Casualty_Class geeft aan of de persoon in kwestie een passagier(2), bestuurder van een voertuig(1) of een voetganger(3) is. Aangezien wij opzoek zijn naar accidenten die werden veroorzaakt door dronken mensen, lijkt het mij onwaarschijnlijk dat passagiers van voertuigen de oorzaak kunnen zijn van een accident dus ik dacht misschien Casualty_Class(2) weglaten?\")\n",
    "joined = joined.where(\"Casualty_Class!= 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f935dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leeftijd\n",
    "joined.groupby(\"Age_of_Casualty\").count().show(20)\n",
    "print(\"Er zitten hier records van kinderen 3, 8 jaar enzo. in die andere excel bestand die jij hebt gestuurd was alle info over 16+ mensen. Dus hier weeral al die kinderen verwijderen? Age_of_Casualty met waarde -1 zijn gevallen waar de leeftijd niet gekend is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = joined.where(\"Age_of_Casualty >= 16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7600c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_age(age):\n",
    "    age = int(age)\n",
    "    if 16 <= age < 25:\n",
    "        return \"16-24\"\n",
    "    elif 25 <= age < 30:\n",
    "        return \"25-29\"\n",
    "    elif 30<=age <35:\n",
    "        return \"30-34\"\n",
    "    elif 35<= age <40:\n",
    "        return \"35-39\"\n",
    "    elif 40<= age <50:\n",
    "        return \"40-49\"\n",
    "    elif 50<= age <60:\n",
    "        return \"50-59\"\n",
    "    else:\n",
    "        return \"+60\"\n",
    "\n",
    "\n",
    "joined = joined.withColumn(\"Age_of_Casualty\", col(\"Age_of_Casualty\").cast(\"int\"))\n",
    "joined = joined.withColumnRenamed(\"1st_Road_Class\", \"First_Road_Class\")\n",
    "joined2 = joined.rdd.map(lambda x: (x.Accident_Index, x.Longitude, \n",
    "                                    x.Latitude, x.Police_Force, x.Accident_Severity, x.Number_of_Vehicles, \n",
    "                                    x.Number_of_Casualties, x.Date, x.Day_of_Week, x.Time, x.First_Road_Class, \n",
    "                                    x.Road_Type, x.Speed_limit, x.Junction_Detail, x.Junction_Control, x.Light_Conditions, \n",
    "                                    x.Weather_Conditions,\n",
    "                                    x.Road_Surface_Conditions, x.Special_Conditions_at_Site,\n",
    "                                    x.Urban_or_Rural_Area, x.Did_Police_Officer_Attend_Scene_of_Accident, x.LSOA_of_Accident_Location, x.Casualty_Class, x.Sex_of_Casualty, x.Age_of_Casualty, x.Casualty_Severity, x.Pedestrian_Location, x.Car_Passenger, x.Casualty_Type, sort_age(x.Age_of_Casualty)))\n",
    "\n",
    "joined2 = joined2.toDF([\"Accident_Index\", \"Longitude\" , \"Latitude\" , \"Police_Force\" , \"Accident_Severity\" ,\n",
    "       \"Number_of_Vehicles\" , \"Number_of_Casualties\" , \"Date\" , \"Day_of_Week\" , \"Time\" , \"1st_Road_Class\" , \"Road_Type\" , \"Speed_limit\" , \"Junction_Detail\" , \"Junction_Control\" , \"Light_Conditions\" , \"Weather_Conditions\" , \"Road_Surface_Conditions\" , \"Special_Conditions_at_Site\" , \"Urban_or_Rural_Area\" ,\n",
    "       \"Did_Police_Officer_Attend_Scene_of_Accident\" , \"LSOA_of_Accident_Location\" , \"Casualty_Class\" , \"Sex_of_Casualty\" , \n",
    "       \"Age_of_Casualty\" , \"Casualty_Severity\" , \"Pedestrian_Location\" , \"Car_Passenger\" , \"Casualty_Type\" , \"Age_Cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined2.show(20, vertical= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542acde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aantal accidenten per leeftijdscategorie\n",
    "cat = joined2.groupby(\"Age_Cat\").count().toDF(\"Age_Cat\", \"count\")\n",
    "age_cat=[]\n",
    "age_cat_amount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cat.collect():\n",
    "    age_cat.append(x[0])\n",
    "    age_cat_amount.append(int(x[1]))\n",
    "print(age_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(age_cat,age_cat_amount)\n",
    "plt.title('Aantal accidenten per leeftijdscategorie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aantal accidenten per Leeftijdscategorie en ernstigheid van het incident\n",
    "cat2 = joined2.groupby(\"Age_Cat\", \"Accident_Severity\").count().toDF(\"Age_Cat\", \"Accident_Severity\", \"count\").orderBy(asc(\"Accident_Severity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cat=[]\n",
    "age_cat_amount=[]\n",
    "severity1=[]\n",
    "severity2=[]\n",
    "severity3=[]\n",
    "amount1=[]\n",
    "amount2=[]\n",
    "amount3=[]\n",
    "for x in cat2.collect():\n",
    "    if x[0] == \"Child\" and x[1]== 1:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"Child\" and x[1]== 2:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"Child\" and x[1]== 3:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"16-24\" and x[1]== 1:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"16-24\" and x[1]== 2:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"16-24\" and x[1]== 3:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"25-34\" and x[1]== 1:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"25-34\" and x[1]== 2:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"25-34\" and x[1]== 3:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"35-44\" and x[1]== 1:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"35-44\" and x[1]== 2:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"35-44\" and x[1]== 3:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"45-60\" and x[1]== 1:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"45-60\" and x[1]== 2:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"45-60\" and x[1]== 3:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"+60\" and x[1]== 1:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"+60\" and x[1]== 2:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"+60\" and x[1]== 3:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"Unknown\" and x[1]== 1:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"Unknown\" and x[1]== 2:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])\n",
    "    elif x[0] == \"Unknown\" and x[1]== 3:\n",
    "        age_cat.append(x[0])\n",
    "        age_cat_amount.append(x[2])\n",
    "        severity.append(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a94611",
   "metadata": {},
   "source": [
    "Kijken naar de accidenten doorheen de tijd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85554f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sqlContext.sql(\"SELECT Day_of_Week from accidentTable\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sqlContext.sql(\"SELECT * from accidentTable\")\n",
    "UK_COORDINATES = (53.5500, -6.4333)\n",
    "map = folium.Map(location=UK_COORDINATES, zoom_start=5)\n",
    "## testing \n",
    "counter=0\n",
    "for indx,row in enumerate(df1.collect()):\n",
    "    print(indx)\n",
    "display(map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
