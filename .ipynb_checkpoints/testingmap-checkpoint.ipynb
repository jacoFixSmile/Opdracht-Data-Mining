{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a87256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from folium) (1.19.5)\n",
      "Requirement already satisfied: branca>=0.3.0 in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from folium) (0.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from folium) (2.25.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from folium) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from requests->folium) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from requests->folium) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from requests->folium) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eigenaar\\anaconda3\\lib\\site-packages (from requests->folium) (2.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\eigenaar\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import findspark\n",
    "import pyspark\n",
    "!pip install folium\n",
    "import folium\n",
    "#path atif  findspark.init(\"C:\\\\spark-3.1.2-bin-hadoop3.2\\\\spark-3.1.2-bin-hadoop3.2\")\n",
    "findspark.init(\"c:/Users/Eigenaar/spark-3.1.2-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb828643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import avg\n",
    "from ipywidgets import widgets\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736bef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating schema's\n",
    "schema_pubs = StructType([\n",
    "    StructField(\"Food Standard Agency's ID\", StringType(), nullable=True),\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"address\", StringType(), nullable=True),\n",
    "    StructField(\"postcode\", StringType(), nullable=True),\n",
    "    StructField(\"easting\", DoubleType(), nullable=True),\n",
    "    StructField(\"northing\", DoubleType(), nullable=True),\n",
    "    StructField(\"latitude\", DoubleType(), nullable=False),\n",
    "    StructField(\"longitude\", DoubleType(), nullable=False),\n",
    "    StructField(\"local_authority\", StringType(), nullable=True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3501cbe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-ff2ac79841c1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-ff2ac79841c1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    importing data\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#importing data \n",
    "#Atif accidentData_df = spark.read.format('csv').option('header',True).load('../../../Desktop/Data mining/Accidents0514.csv')\n",
    "accidentData_df = spark.read.format('csv').option('header',True).load('Kaggle_datasets/Accidents0514.csv')\n",
    "pubsData_df = spark.read.format('csv').option('header',False).schema(schema_pubs).load('open_pubs.csv')\n",
    "### casualtiesData_df geeft meer info over het slachtoffer\n",
    "#atif casualtiesData_df = spark.read.format('csv').option('header', True).load('../../../Desktop/Data mining/Casualties0514.csv')\n",
    "casualtiesData_df = spark.read.format('csv').option('header', True).load('Kaggle_datasets/Casualties0514.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24740a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21316af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploratie\n",
    "### Filter de pub data set\n",
    "counterRowsBeforePubs=pubsData_df.count()\n",
    "pubsData_df=pubsData_df.dropna(subset=[\"latitude\",\"longitude\"])\n",
    "print(counterRowsBeforePubs-pubsData_df.count(),\" rijen zijn verwijderd omdat ze lege waarden hadden.\")\n",
    "### nutteloze kolommen zoals Pedestrian_Crossing,Special_Condition_at_site, Carriageway_Hazards,  want volgens Kaggle hebben bijna alle records dezelfde data dus veel gaat deze kolom ons niet opleveren alleen maar een langere run time. heb ook Location_Northing_OSGR, Location_Easting_OSGR, Local_Authority_(District), Local_Authority_(Highway)1st_Road_Class, 1st_Road_Number, 2nd_Road_Class, 2nd_Road_Number, Pedestrian_Crossing-Human_Control, Pedestrian_Crossing-Physical_Facilities\n",
    "accidentData_df = accidentData_df.drop(\"Pedestrian_Crossing\", \"Special_Condition_at_site\",\"Local_Authority_(Highway)\", \"Carriageway_Hazards\",\"Location_Northing_OSGR\", \"Location_Easting_OSGR\", \"Local_Authority_(District)\", \"Local_Authority_(Highway)1st_Road_Class\", \"1st_Road_Number\", \"2nd_Road_Class\", \"2nd_Road_Number\", \"Pedestrian_Crossing-Human_Control\", \"Pedestrian_Crossing-Physical_Facilities\")\n",
    "### In casualtiesData heb ik ook een paar kolommen verwijderd => Pedestian_Location, Pedestrian_Movement, Bus_or_Coach_, Vehicle_Reference, Casualty_Reference, Age_Band_of_Casualty, Pedestrian_Road_Maintenance_Worker, Casualty_Home_Area_Type\n",
    "casualtiesData_df = casualtiesData_df.drop(\"Pedestian_Location\", \"Pedestrian_Movement\", \"Bus_or_Coach_Passenger\", \"Vehicle_Reference\", \"Casualty_Reference\", \"Age_Band_of_Casualty\", \"Pedestrian_Road_Maintenance_Worker\",\"Casualty_Home_Area_Type\" )\n",
    "print(\"Kolommen Vehicle_Reference en Casualty_Reference zijn zeer nuttige gegevens maar die waarden komen precies niet overeen met die tabel op Kaggle dus heb ze weggelaten. Kan dat ik verkeerd ben maar heb niets gevonden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter de accident data set\n",
    "counterRowsBeforePubs=accidentData_df.count()\n",
    "accidentData_df=accidentData_df.na.drop()\n",
    "print(counterRowsBeforePubs-accidentData_df.count(),\" rijen zijn verwijderd omdat ze lege waarden hadden.\")\n",
    "### Filter de casaulties data\n",
    "counterRowsBeforeCasaul=casualtiesData_df.count()\n",
    "casualtiesData_df=casualtiesData_df.na.drop()\n",
    "print(counterRowsBeforeCasaul-casualtiesData_df.count(),\" rijen zijn verwijderd omdat ze lege waarden hadden.\")\n",
    "accidentData_df.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "### twee dataframes in één dataframe joinen m.b.v. ID(Accident_Index). Beide dataframes hebben evenveel records.\n",
    "joined = accidentData_df.alias(\"A\").join(casualtiesData_df.alias(\"B\"),col(\"A.Accident_Index\") == col(\"B.Accident_Index\"),\"inner\").select(col(\"A.Accident_Index\"), col(\"A.Longitude\"),col(\"A.Latitude\"), col(\"A.Police_Force\"), col(\"A.Accident_Severity\"), col(\"A.Number_of_Vehicles\"), col(\"A.Number_of_Casualties\"), col(\"A.Date\"), col(\"A.Day_of_Week\"),col(\"A.Time\"), col(\"A.1st_Road_Class\"), col(\"A.Road_Type\"), col(\"A.Speed_limit\"), col(\"A.Junction_Detail\"), col(\"A.Junction_Control\"), col(\"A.Light_Conditions\"), col(\"A.Weather_Conditions\"), col(\"A.Road_Surface_Conditions\"), col(\"A.Special_Conditions_at_Site\"), col(\"A.Urban_or_Rural_Area\"), col(\"A.Did_Police_Officer_Attend_Scene_of_Accident\"), col(\"A.LSOA_of_Accident_Location\"), col(\"B.Casualty_Class\"), col(\"B.Sex_of_Casualty\"), col(\"B.Age_of_Casualty\"), col(\"B.Casualty_Severity\"), col(\"B.Pedestrian_Location\"), col(\"B.Car_Passenger\"), col(\"B.Casualty_Type\")).toDF(\"Accident_Index\", \"Longitude\",\"Latitude\",\"Police_Force\", \"Accident_Severity\",\"Number_of_Vehicles\", \"Number_of_Casualties\", \"Date\", \"Day_of_Week\",\"Time\",\"1st_Road_Class\", \"Road_Type\", \"Speed_limit\",\"Junction_Detail\", \"Junction_Control\",\"Light_Conditions\", \"Weather_Conditions\",\"Road_Surface_Conditions\",\"Special_Conditions_at_Site\",\"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\", \"LSOA_of_Accident_Location\",\"Casualty_Class\",\"Sex_of_Casualty\",\"Age_of_Casualty\",\"Casualty_Severity\",\"Pedestrian_Location\",\"Car_Passenger\",\"Casualty_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39745992",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Soorten voertuigen\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "joined.groupby(\"Casualty_Type\").count().show(22)\n",
    "print(\"Ongeveer 83% van alle accidenten werden veroorzaakt in een auto (Casualty_Type 9). Andere voertuigen zijn vooral brommers onderverdeeld in groepen (50cc, 125cc, 500cc enzo) andere voertuigen zijn wel een beetje raar boerderij voertuigen? taxi? trambestuurders? zelfs paarden. Kans dat er iemand met een paard naar een pub gaat is vrij klein denk ik. Dus wat doen wij? enkel autos en brommers bijhouden en de rest verwijderen of gewoon zo laten?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Casualty Class\n",
    "joined.groupby(\"Casualty_Class\").count().show()\n",
    "print(\"Casualty_Class geeft aan of de persoon in kwestie een passagier(2), bestuurder van een voertuig(1) of een voetganger(3) is. Aangezien wij opzoek zijn naar accidenten die werden veroorzaakt door dronken mensen, lijkt het mij onwaarschijnlijk dat passagiers van voertuigen de oorzaak kunnen zijn van een accident dus ik dacht misschien Casualty_Class(2) weglaten?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5277ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leeftijd\n",
    "joined.groupby(\"Age_of_Casualty\").count().show(121)\n",
    "print(\"Er zitten hier records van kinderen 3, 8 jaar enzo. in die andere excel bestand die jij hebt gestuurd was alle info over 16+ mensen. Dus hier weeral al die kinderen verwijderen? Age_of_Casualty met waarde -1 zijn gevallen waar de leeftijd niet gekend is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18087c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_age(age):\n",
    "    age = int(age)\n",
    "    if 16 <= age < 25:\n",
    "        return \"16-24\"\n",
    "    elif 25<=age <35:\n",
    "        return \"25-34\"\n",
    "    elif 35<= age <45:\n",
    "        return \"35-44\"\n",
    "    elif 45<= age <60:\n",
    "        return \"45-60\"\n",
    "    elif 60<= age <120:\n",
    "        return \"+60\"\n",
    "    elif 0 <= age <16:\n",
    "        return \"Child\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "sort_age(24)\n",
    "joined = joined.withColumn(\"Age_of_Casualty\", col(\"Age_of_Casualty\").cast(\"int\"))\n",
    "joined = joined.withColumnRenamed(\"1st_Road_Class\", \"First_Road_Class\")\n",
    "joined2 = joined.rdd.map(lambda x: (x.Accident_Index, x.Longitude, \n",
    "                                    x.Latitude, x.Police_Force, x.Accident_Severity, x.Number_of_Vehicles, \n",
    "                                    x.Number_of_Casualties, x.Date, x.Day_of_Week, x.Time, x.First_Road_Class, \n",
    "                                    x.Road_Type, x.Speed_limit, x.Junction_Detail, x.Junction_Control, x.Light_Conditions, \n",
    "                                    x.Weather_Conditions,\n",
    "                                    x.Road_Surface_Conditions, x.Special_Conditions_at_Site,\n",
    "                                    x.Urban_or_Rural_Area, x.Did_Police_Officer_Attend_Scene_of_Accident, x.LSOA_of_Accident_Location, x.Casualty_Class, x.Sex_of_Casualty, x.Age_of_Casualty, x.Casualty_Severity, x.Pedestrian_Location, x.Car_Passenger, x.Casualty_Type, sort_age(x.Age_of_Casualty)))\n",
    "\n",
    "joined2 = joined2.toDF([\"Accident_Index\", \"Longitude\" , \"Latitude\" , \"Police_Force\" , \"Accident_Severity\" ,\n",
    "       \"Number_of_Vehicles\" , \"Number_of_Casualties\" , \"Date\" , \"Day_of_Week\" , \"Time\" , \"1st_Road_Class\" , \"Road_Type\" , \"Speed_limit\" , \"Junction_Detail\" , \"Junction_Control\" , \"Light_Conditions\" , \"Weather_Conditions\" , \"Road_Surface_Conditions\" , \"Special_Conditions_at_Site\" , \"Urban_or_Rural_Area\" ,\n",
    "       \"Did_Police_Officer_Attend_Scene_of_Accident\" , \"LSOA_of_Accident_Location\" , \"Casualty_Class\" , \"Sex_of_Casualty\" , \n",
    "       \"Age_of_Casualty\" , \"Casualty_Severity\" , \"Pedestrian_Location\" , \"Car_Passenger\" , \"Casualty_Type\" , \"Age_Cat\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined2.show(20, vertical= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37611294",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c027132",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined2.registerTempTable('joined2')\n",
    "qr = sqlContext.sql(\"SELECT COUNT()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eadc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aantal accidenten per leeftijdscategorie\n",
    "joined2.groupby(\"Age_Cat\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aantal accidenten per Leeftijdscategorie en ernstigheid van het incident\n",
    "joined2.groupby(\"Age_Cat\", \"Accident_Severity\").count().show()\n",
    "print(\"Zal proberen om er een staafdiagram van te maken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efee5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
